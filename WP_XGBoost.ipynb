{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Kfold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import XGBoost\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set viewing options\n",
    "pd.set_option('display.width', 1000)  # Set the maximum width of the display\n",
    "pd.set_option('display.max_columns', None)  # Display all columns without truncation\n",
    "np.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide   density        pH  sulphates   alcohol  wine_type\n",
      "0       0.264463          0.126667     0.216867        0.308282   0.059801             0.152778              0.377880  0.267785  0.217054   0.129213  0.115942        1.0\n",
      "1       0.206612          0.146667     0.204819        0.015337   0.066445             0.045139              0.290323  0.132832  0.449612   0.151685  0.217391        1.0\n",
      "2       0.355372          0.133333     0.240964        0.096626   0.068106             0.100694              0.209677  0.154039  0.418605   0.123596  0.304348        1.0\n",
      "3       0.280992          0.100000     0.192771        0.121166   0.081395             0.159722              0.414747  0.163678  0.364341   0.101124  0.275362        1.0\n",
      "4       0.198347          0.160000     0.096386        0.098160   0.059801             0.100694              0.299539  0.150183  0.356589   0.140449  0.231884        1.0\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.read_csv('X_df_normalized.csv', sep= ',')\n",
    "y_df = pd.read_csv('y_df.csv', sep = ',')\n",
    "whole_csv = pd.read_csv('whole_csv.csv')\n",
    "X_df['wine_type'] = whole_csv['wine_type_white']\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      quality_label_encoded\n",
      "2629                      1\n",
      "511                       0\n",
      "62                        0\n",
      "3128                      1\n",
      "4233                      1\n",
      "...                     ...\n",
      "3092                      0\n",
      "3772                      1\n",
      "5191                      1\n",
      "5226                      0\n",
      "860                       2\n",
      "\n",
      "[4256 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, x_test, y_train, y_test = train_test_split( X_df, y_df, test_size = 0.2, random_state = 42)\n",
    "y_train_int = y_train.astype(int)\n",
    "y_test_int = y_test.astype(int)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[18:25:39] /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:144: label must be in [0,1] for logistic regression\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001655ec998 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000001657cf788 xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float>>*) + 580\n  [bt] (2) 3   libxgboost.dylib                    0x000000016579af60 xgboost::obj::FitIntercept::InitEstimation(xgboost::MetaInfo const&, xgboost::linalg::Tensor<float, 1>*) const + 384\n  [bt] (3) 4   libxgboost.dylib                    0x0000000165746c3c xgboost::LearnerConfiguration::InitBaseScore(xgboost::DMatrix const*) + 264\n  [bt] (4) 5   libxgboost.dylib                    0x00000001657344bc xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 140\n  [bt] (5) 6   libxgboost.dylib                    0x000000016560cea8 XGBoosterUpdateOneIter + 144\n  [bt] (6) 7   libffi.8.dylib                      0x000000010793004c ffi_call_SYSV + 76\n  [bt] (7) 8   libffi.8.dylib                      0x000000010792d834 ffi_call_int + 1404\n  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x0000000107910150 _ctypes_callproc + 752\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mbinary:logistic\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# for binary classification\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39meval_metric\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlogloss\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m num_rounds \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m  \u001b[39m# you can adjust the number of boosting rounds\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m xgb_model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mtrain(params, dtrain, num_rounds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/WP_XGBoost.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m predictions \u001b[39m=\u001b[39m xgb_model\u001b[39m.\u001b[39mpredict(dtest)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2049\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2049\u001b[0m     _check_call(\n\u001b[1;32m   2050\u001b[0m         _LIB\u001b[39m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2051\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, ctypes\u001b[39m.\u001b[39mc_int(iteration), dtrain\u001b[39m.\u001b[39mhandle\n\u001b[1;32m   2052\u001b[0m         )\n\u001b[1;32m   2053\u001b[0m     )\n\u001b[1;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:281\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [18:25:39] /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:144: label must be in [0,1] for logistic regression\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001655ec998 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000001657cf788 xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float>>*) + 580\n  [bt] (2) 3   libxgboost.dylib                    0x000000016579af60 xgboost::obj::FitIntercept::InitEstimation(xgboost::MetaInfo const&, xgboost::linalg::Tensor<float, 1>*) const + 384\n  [bt] (3) 4   libxgboost.dylib                    0x0000000165746c3c xgboost::LearnerConfiguration::InitBaseScore(xgboost::DMatrix const*) + 264\n  [bt] (4) 5   libxgboost.dylib                    0x00000001657344bc xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 140\n  [bt] (5) 6   libxgboost.dylib                    0x000000016560cea8 XGBoosterUpdateOneIter + 144\n  [bt] (6) 7   libffi.8.dylib                      0x000000010793004c ffi_call_SYSV + 76\n  [bt] (7) 8   libffi.8.dylib                      0x000000010792d834 ffi_call_int + 1404\n  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x0000000107910150 _ctypes_callproc + 752\n\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train_int)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test_int)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # for binary classification\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "num_rounds = 100  # you can adjust the number of boosting rounds\n",
    "xgb_model = xgb.train(params, dtrain, num_rounds)\n",
    "predictions = xgb_model.predict(dtest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
