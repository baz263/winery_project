{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' statements to import libraries'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import kurtosis\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pd.set_option('display.width', 1000)  # Set the maximum width of the display\n",
    "pd.set_option('display.max_columns', None)  # Display all columns without truncation\n",
    "np.set_printoptions(linewidth=150)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  alcohol  quality\n",
      "0            7.0              0.27         0.36            20.7      0.045                 45.0                 170.0   1.0010  3.00       0.45      8.8        6\n",
      "1            6.3              0.30         0.34             1.6      0.049                 14.0                 132.0   0.9940  3.30       0.49      9.5        6\n",
      "2            8.1              0.28         0.40             6.9      0.050                 30.0                  97.0   0.9951  3.26       0.44     10.1        6\n",
      "3            7.2              0.23         0.32             8.5      0.058                 47.0                 186.0   0.9956  3.19       0.40      9.9        6\n",
      "4            7.2              0.23         0.32             8.5      0.058                 47.0                 186.0   0.9956  3.19       0.40      9.9        6\n",
      "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  alcohol  quality\n",
      "0            7.0              0.27         0.36            20.7      0.045                 45.0                 170.0   1.0010  3.00       0.45      8.8        6\n",
      "1            6.3              0.30         0.34             1.6      0.049                 14.0                 132.0   0.9940  3.30       0.49      9.5        6\n",
      "2            8.1              0.28         0.40             6.9      0.050                 30.0                  97.0   0.9951  3.26       0.44     10.1        6\n",
      "3            7.2              0.23         0.32             8.5      0.058                 47.0                 186.0   0.9956  3.19       0.40      9.9        6\n",
      "6            6.2              0.32         0.16             7.0      0.045                 30.0                 136.0   0.9949  3.18       0.47      9.6        6\n"
     ]
    }
   ],
   "source": [
    "'''statements to add csv files, remove duplicates etc. Just Red and white individually\n",
    "'''\n",
    "#White wine\n",
    "white_df = pd.read_csv('wine+quality/winequality-white.csv', sep= ';')\n",
    "white_copy = white_df.copy()\n",
    "white_copy.columns = white_copy.columns.str.replace(' ', '_')\n",
    "print(white_copy.head())\n",
    "\n",
    "white_copy = white_copy.drop_duplicates()\n",
    "print(white_copy.head())\n",
    "\n",
    "if 'colour' not in white_copy.columns:\n",
    "    white_copy['colour'] = 'white'\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#Red wine\n",
    "red_df = pd.read_csv('wine+quality/winequality-red.csv', sep = ';')\n",
    "red_copy = red_df.copy()\n",
    "if 'colour' not in red_copy.columns:\n",
    "    red_copy['colour'] = 'red'\n",
    "else:\n",
    "    pass\n",
    "\n",
    "red_copy.columns = red_copy.columns.str.replace(' ', '_')\n",
    "red_copy = red_copy.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' add new column to wine_quality\n",
    "low <= 5\n",
    "5 < medium < 7\n",
    "high >= 7\n",
    "'''\n",
    "if 'quality_label' not in white_copy.columns:\n",
    "    white_copy['quality_label'] = white_copy.quality.apply(lambda value: 'low' if value <= 5 else 'medium' if value <= 7 else 'high')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "if 'quality_label' not in red_copy.columns:\n",
    "    red_copy['quality_label'] = red_copy.quality.apply(lambda value: 'low' if value <= 5 else 'medium' if value <= 7 else 'high')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "'''add new Column wine_type'''\n",
    "if 'wine_type' not in white_copy.columns:\n",
    "    white_copy['wine_type'] = white_copy.quality.apply(lambda value: 'white')\n",
    "else:\n",
    "    pass\n",
    "if 'wine_type' not in red_copy.columns:\n",
    "    red_copy['wine_type'] = red_copy.quality.apply(lambda value: 'red')\n",
    "else:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''making Good Medium Bad have a specified order'''\n",
    "# category_order = ['low', 'medium', 'high']\n",
    "# red_copy['quality_label'] = red_copy['quality_label'].astype(CategoricalDtype(categories=category_order, ordered=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''combining DFs and creating white_red_df'''\n",
    "\n",
    "white_red_df = pd.concat([white_copy, red_copy])\n",
    "white_red_copy = white_red_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = white_red_df.quality_label.value_counts()\n",
    "plt.figure(figsize= (3,3))\n",
    "sns.set(style = 'whitegrid')\n",
    "plt.pie(label_counts, labels = label_counts.index, autopct = '%1.1f%%', startangle = 90)\n",
    "plt.title('Distribution of low, medium, hight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>colour</th>\n",
       "      <th>quality_label</th>\n",
       "      <th>wine_type_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "      <td>white</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  alcohol  quality colour quality_label  wine_type_white\n",
       "0            7.0              0.27         0.36            20.7      0.045                 45.0                 170.0   1.0010  3.00       0.45      8.8        6  white        medium              1.0\n",
       "1            6.3              0.30         0.34             1.6      0.049                 14.0                 132.0   0.9940  3.30       0.49      9.5        6  white        medium              1.0\n",
       "2            8.1              0.28         0.40             6.9      0.050                 30.0                  97.0   0.9951  3.26       0.44     10.1        6  white        medium              1.0\n",
       "3            7.2              0.23         0.32             8.5      0.058                 47.0                 186.0   0.9956  3.19       0.40      9.9        6  white        medium              1.0\n",
       "6            6.2              0.32         0.16             7.0      0.045                 30.0                 136.0   0.9949  3.18       0.47      9.6        6  white        medium              1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''lets encode the data'''\n",
    "encoder = OneHotEncoder(handle_unknown= 'ignore', sparse_output= False).set_output(transform= 'pandas')\n",
    "\n",
    "colour_encoded_data = encoder.fit_transform(white_red_copy[['wine_type']])\n",
    "if 'wine_type' in white_red_copy.columns:\n",
    "    ml_df = pd.concat([white_red_copy, colour_encoded_data], axis= 1)\n",
    "#print(white_red_copy.columns)\n",
    "#ml_df = white_red_copy[['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar','chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n",
    "       #'pH', 'sulphates', 'alcohol', 'quality','wine_type_red']]\n",
    "if 'wine_type' in ml_df.columns:\n",
    "    ml_df.drop('wine_type', axis=1, inplace= True)\n",
    "    ml_df.drop('wine_type_red', axis = 1, inplace = True)\n",
    "    #ml_df.drop(['quality', 'quality_label', 'colour'], axis=1, inplace= True)\n",
    "display (ml_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['low', 'medium', 'high']], dtype='<U6')"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['low', 'medium', 'high']\n",
    "categories = np.reshape(categories, (1, -1))\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  alcohol  quality colour  quality_label  wine_type_white\n",
      "1593            6.8             0.620         0.08             1.9      0.068                 28.0                  38.0  0.99651  3.42       0.82      9.5        6    red              1              0.0\n",
      "1594            6.2             0.600         0.08             2.0      0.090                 32.0                  44.0  0.99490  3.45       0.58     10.5        5    red              0              0.0\n",
      "1595            5.9             0.550         0.10             2.2      0.062                 39.0                  51.0  0.99512  3.52       0.76     11.2        6    red              1              0.0\n",
      "1597            5.9             0.645         0.12             2.0      0.075                 32.0                  44.0  0.99547  3.57       0.71     10.2        5    red              0              0.0\n",
      "1598            6.0             0.310         0.47             3.6      0.067                 18.0                  42.0  0.99549  3.39       0.66     11.0        6    red              1              0.0\n"
     ]
    }
   ],
   "source": [
    "''' lets use label encoder to encode the wine_quality\n",
    "we cannot use onehot encoder as that only does binary, dichotomous choices\n",
    "low 1\n",
    "medium 2\n",
    "high 0\n",
    "'''\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# categories = ['low', 'medium', 'high']\n",
    "# categories = np.reshape(categories, (1, -1))\n",
    "# oe = OrdinalEncoder(categories = ['low', 'medium', 'high'])\n",
    "# ml_df['quality_label_encoded'] = oe.fit_transform(ml_df['quality_label'])\n",
    "# print(ml_df.head(20))\n",
    "# display(ml_df.info())\n",
    "\n",
    "quality_codes = {'low' : 0, 'medium': 1, 'high':2}\n",
    "ml_df['quality_label'].replace(quality_codes, inplace= True)\n",
    "print(ml_df.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>wine_type_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  alcohol  wine_type_white\n",
       "0            7.0              0.27         0.36            20.7      0.045                 45.0                 170.0   1.0010  3.00       0.45      8.8              1.0\n",
       "1            6.3              0.30         0.34             1.6      0.049                 14.0                 132.0   0.9940  3.30       0.49      9.5              1.0\n",
       "2            8.1              0.28         0.40             6.9      0.050                 30.0                  97.0   0.9951  3.26       0.44     10.1              1.0\n",
       "3            7.2              0.23         0.32             8.5      0.058                 47.0                 186.0   0.9956  3.19       0.40      9.9              1.0\n",
       "6            6.2              0.32         0.16             7.0      0.045                 30.0                 136.0   0.9949  3.18       0.47      9.6              1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' drop other unwanted columns: quality_label, colour, quality'''\n",
    "for val in ['quality_label', 'colour', 'quality', 'wine_type']:\n",
    "    if val in ml_df.columns:\n",
    "        ml_df.drop(val, axis =1, inplace= True)\n",
    "display(ml_df.head())\n",
    "ml_df2 = ml_df.copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      wine_type_white\n",
      "3207              1.0\n",
      "612               1.0\n",
      "71                1.0\n",
      "3857              1.0\n",
      "312               0.0\n",
      "      fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  alcohol\n",
      "3207            6.8              0.21         0.42             1.2      0.045                 24.0                 126.0  0.99234  3.09       0.87     10.9\n",
      "612             7.5              0.23         0.68            11.0      0.047                 37.0                 133.0  0.99780  2.99       0.38      8.8\n",
      "71              6.8              0.30         0.23             4.6      0.061                 50.5                 238.5  0.99580  3.32       0.60      9.5\n",
      "3857            5.4              0.17         0.27             2.7      0.049                 28.0                 104.0  0.99224  3.46       0.55     10.3\n",
      "312             9.0              0.46         0.31             2.8      0.093                 19.0                  98.0  0.99815  3.32       0.63      9.5\n"
     ]
    }
   ],
   "source": [
    "''' split the data'''\n",
    "X_train, x_test, Y_train, y_test = train_test_split( ml_df.iloc[:,:-1],ml_df.iloc[:, -1:], test_size = 0.2, random_state = 42)\n",
    "# display(X_train.shape)\n",
    "# display(x_test.shape)\n",
    "# display(Y_train.shape)\n",
    "# display(y_test.shape)\n",
    "print(Y_train.head())\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.21"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_regress(X_train, Y_train, x_test, y_test, graph):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    Y_train = np.ravel(Y_train)\n",
    "    print(Y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    lr = LogisticRegression(random_state = 42, max_iter = 10000, multi_class= 'multinomial')\n",
    "    lr.fit(X_train, Y_train)\n",
    "    pred_lr = lr.predict(x_test)\n",
    "    confu_mat = confusion_matrix(y_test, pred_lr)\n",
    "    accuracy_percentage = round(accuracy_score(y_test, pred_lr) * 100,2)\n",
    "    #print(\"Accuracy of Logistic Regression:\", accuracy_percentage, \"%\")\n",
    "    \n",
    "#printing a graph of the heatmap\n",
    "    if graph == True:\n",
    "        sns.heatmap(confu_mat, annot=True, fmt= 'g', cmap = 'Blues')\n",
    "        plt.suptitle('Logistic Regression Confusion Map')        \n",
    "        plt.show()\n",
    "    #rint('________________')\n",
    "    return accuracy_percentage\n",
    "log_regress(X_train, Y_train, x_test, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''neighbours classifier''' \n",
    "def neigh_class(X_train, Y_train, x_test, y_test, graph):\n",
    "    Y_train = np.ravel(Y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    accuracy_percentage = (knn.score(x_test, y_test) * 100).round(2)\n",
    "    #print(\"Accuracy of Neighbours Classifier:\", accuracy_percentage, \"%\")\n",
    "    pred_neighbours = knn.predict(x_test)\n",
    "    confu_mat = confusion_matrix(y_test, pred_neighbours)    \n",
    "    if graph == True:\n",
    "        sns.heatmap(confu_mat, annot=True, fmt= 'g', cmap = 'Blues')\n",
    "        plt.suptitle('Random Forest Confusion Map')\n",
    "        plt.show()\n",
    "    #print('________________')\n",
    "    return accuracy_percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.84"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''decision tree'''\n",
    "def dec_tree(X_train, Y_train, x_test, y_test, graph):\n",
    "    \n",
    "    Y_train = np.ravel(Y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "    tree.fit(X_train, Y_train)\n",
    "    accuracy_percentage = (tree.score(x_test, y_test) * 100).round(2)\n",
    "    #print(\"Accuracy of Decision Tree:\", accuracy_percentage, \"%\")\n",
    "    pred_tree = tree.predict(x_test)\n",
    "    confu_mat = confusion_matrix(y_test, pred_tree)\n",
    "    plt.show()\n",
    "    if graph == True:\n",
    "        sns.heatmap(confu_mat, annot=True, fmt= 'g', cmap = 'Blues')\n",
    "        plt.suptitle('Decision Tree Confusion Map')\n",
    "        plt.show()\n",
    "    \n",
    "    #print('________________')\n",
    "    return accuracy_percentage\n",
    "\n",
    "dec_tree(X_train, Y_train, x_test, y_test, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''random forest'''\n",
    "def ran_forest(X_train, Y_train, x_test, y_test, graph) :\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    Y_train = np.ravel(Y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 10)\n",
    "    forest.fit(X_train, Y_train)\n",
    "    accuracy_percentage = (forest.score(x_test, y_test) * 100).round(2)\n",
    "    pred_forest = forest.predict(x_test)\n",
    "    confu_mat = confusion_matrix(y_test, pred_forest)\n",
    "    \n",
    "    #print(\"Accuracy of Random Forest:\", accuracy_percentage, \"%\")    \n",
    "    if graph == True:\n",
    "        sns.heatmap(confu_mat, annot=True, fmt= 'g', cmap = 'Blues')\n",
    "        plt.suptitle('Random Forest Confusion Map')\n",
    "        plt.show()\n",
    "    #print('________________')\n",
    "    return accuracy_percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Linear regression'''\n",
    "def lin_reg(X_train, Y_train, x_test, y_test):\n",
    "    Y_train = np.ravel(Y_train)\n",
    "    y_test = np.ravel(y_test)\n",
    "    from sklearn import linear_model\n",
    "    linreg = linear_model.LinearRegression()\n",
    "    linreg.fit(X_train, Y_train)\n",
    "    accuracy_percentage = (linreg.score(x_test, y_test) * 100).round(2)\n",
    "    pred_linreg = linreg.predict(x_test)\n",
    "\n",
    "    #print(\"Accuracy of Linear Regression:\", accuracy_percentage, \"%\")\n",
    "    #print('________________')\n",
    "    return accuracy_percentage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>98.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>85.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neighbours Classifier</th>\n",
       "      <td>94.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>97.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>99.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       all_variables\n",
       "Logistic Regression            98.21\n",
       "Linear Regression              85.02\n",
       "Neighbours Classifier          94.08\n",
       "Decision Tree                  97.84\n",
       "Random Forest                  99.06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_names = ['Logistic Regression', 'Linear Regression', 'Neighbours Classifier', 'Decision Tree', 'Random Forest']\n",
    "comparison_df = pd.DataFrame(index=index_names)\n",
    "#ml_df = ml_df.drop(['quality'], axis = 1)X_train, x_test, Y_train, y_test = train_test_split( ml_df.iloc[:,:-1],ml_df.iloc[:, -1:], test_size = 0.2, random_state = 42)\n",
    "\n",
    "comparison_df['all_variables'] = [log_regress(X_train, Y_train, x_test, y_test, False),\n",
    "lin_reg(X_train, Y_train, x_test, y_test),\n",
    "neigh_class(X_train, Y_train, x_test, y_test, False),\n",
    "dec_tree(X_train, Y_train, x_test, y_test, False),\n",
    "ran_forest(X_train, Y_train, x_test, y_test, False)]\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'quality_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'quality_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/barry/CodeAcademy/winery_project/ML_02.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m''' create a correlation matrix between quality_label and the other variables\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mthen save it as a sorted list where the names are in order\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msorted list of \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m corr \u001b[39m=\u001b[39m ml_df\u001b[39m.\u001b[39mcorr()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m corr_list \u001b[39m=\u001b[39m corr[\u001b[39m'\u001b[39m\u001b[39mquality_label\u001b[39m\u001b[39m'\u001b[39m][:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m corr_list \u001b[39m=\u001b[39m corr_list\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mabs\u001b[39m(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barry/CodeAcademy/winery_project/ML_02.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m corr_list \u001b[39m=\u001b[39m corr_list\u001b[39m.\u001b[39msort_values()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'quality_label'"
     ]
    }
   ],
   "source": [
    "''' create a correlation matrix between quality_label and the other variables\n",
    "then save it as a sorted list where the names are in order\n",
    "sorted list of \n",
    "\n",
    "'''\n",
    "corr = ml_df.corr()\n",
    "\n",
    "corr_list = corr['quality_label'][:-1]\n",
    "\n",
    "corr_list = corr_list.apply(lambda x: abs(x))\n",
    "corr_list = corr_list.sort_values()\n",
    "corr_list_names = corr_list.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create my DF showing the results of my algorithms. WE will start with all columsn'''\n",
    "comparison_df = pd.DataFrame(index=index_names)\n",
    "comparison_df['all'] = [log_regress(X_train, Y_train, x_test, y_test, False),\n",
    "lin_reg(X_train, Y_train, x_test, y_test),\n",
    "neigh_class(X_train, Y_train, x_test, y_test, False),\n",
    "dec_tree(X_train, Y_train, x_test, y_test, False),\n",
    "ran_forest(X_train, Y_train, x_test, y_test, False)]\n",
    "display(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ml_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' using a for loop we will remove one column at a time based on the columns correlation with our target depenedent variable\n",
    "using our corr_list_names variable '''\n",
    "for val in corr_list_names[:-1]:\n",
    "    ml_df = ml_df.drop([val], axis = 1)\n",
    "    X_train, x_test, Y_train, y_test = train_test_split( ml_df.iloc[:,:-1],ml_df.iloc[:, -1:], test_size = 0.2, random_state = 42)\n",
    "    comparison_df[val] = [log_regress(X_train, Y_train, x_test, y_test, False),\n",
    "    lin_reg(X_train, Y_train, x_test, y_test),\n",
    "    neigh_class(X_train, Y_train, x_test, y_test, False),\n",
    "    dec_tree(X_train, Y_train, x_test, y_test, False),\n",
    "    ran_forest(X_train, Y_train, x_test, y_test, False)]\n",
    "\n",
    "#backwards elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''adding a style to highlight teh highest value in each row'''\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: lightyellow' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "comparison_df = comparison_df.style.apply(highlight_max, axis = 1)\n",
    "\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work out evaluation metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division = 1)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Playing with K-folds: decision tree'''\n",
    "\n",
    "def dec_tree_kfold(X, Y):\n",
    "    Y = np.ravel(Y)\n",
    "    #creating an instance of the decision Tree classifier\n",
    "    tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "    #how many 'folds' do we want\n",
    "    n_folds = 3\n",
    "    #initialize the StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits= n_folds, shuffle = True, random_state=42)\n",
    "    \n",
    "    #used to hold results from each loop \n",
    "    accuracy_list, precision_list, recall_list, f1_list, kappa_list = [], [], [], [], []\n",
    "\n",
    "    #because n_folds= 5 this should run 5 times\n",
    "    #it will return two lists of indexes - 1 for the training: X and 1 for the test: Y\n",
    "    for train, test in skf.split(X ,Y):\n",
    "        #we isolate the appropriate indexes for our training and our testing\n",
    "        X_train_k, X_test_k = X.loc[train], X.loc[test]\n",
    "        y_train_k, y_test_k = Y[train], Y[test]\n",
    "        #we enter our training data in to the instance of DecisionTreeClassifier we created\n",
    "        tree.fit(X_train_k, y_train_k)\n",
    "        #we test our model by entering our test X ie. the DF with our variables we have not used so far\n",
    "        y_pred = tree.predict(X_test_k)\n",
    "        #use the inbuilt compute_metrics to get the accuracy, precision, recall and f1 stats and add them to our container lists\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_test_k, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "        kappa_score = cohen_kappa_score(y_test_k, y_pred)\n",
    "        kappa_list.append(kappa_score)\n",
    "\n",
    "\n",
    "\n",
    "    #get the average of each our run throughs\n",
    "    accuracy = np.mean(accuracy_list)\n",
    "    precision = np.mean(accuracy_list)\n",
    "    recall = np.mean(recall_list)\n",
    "    f1 = np.mean(f1_list)\n",
    "    kappa = np.mean(kappa_list)\n",
    "    return [accuracy, precision, recall, f1, kappa]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_kfold(X, Y):\n",
    "    Y = np.ravel(Y)\n",
    "    #creating an instance of the decision Tree classifier\n",
    "    tree = RandomForestClassifier(criterion = 'entropy', random_state = 0)\n",
    "    #how many 'folds' do we want\n",
    "    n_folds = 3\n",
    "    #initialize the StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits= n_folds, shuffle = True, random_state=42)\n",
    "    \n",
    "    #used to hold results from each loop \n",
    "    accuracy_list, precision_list, recall_list, f1_list, kappa_list = [], [], [], [], []\n",
    "\n",
    "    #because n_folds= 5 this should run 5 times\n",
    "    #it will return two lists of indexes - 1 for the training: X and 1 for the test: Y\n",
    "    for train, test in skf.split(X ,Y):\n",
    "        #we isolate the appropriate indexes for our training and our testing\n",
    "        X_train_k, X_test_k = X.loc[train], X.loc[test]\n",
    "        y_train_k, y_test_k = Y[train], Y[test]\n",
    "        #we enter our training data in to the instance of DecisionTreeClassifier we created\n",
    "        tree.fit(X_train_k, y_train_k)\n",
    "        #we test our model by entering our test X ie. the DF with our variables we have not used so far\n",
    "        y_pred = tree.predict(X_test_k)\n",
    "        #use the inbuilt compute_metrics to get the accuracy, precision, recall and f1 stats and add them to our container lists\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_test_k, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        kappa_score = cohen_kappa_score(y_test_k, y_pred)\n",
    "        kappa_list.append(kappa_score)\n",
    "\n",
    "    #get the average of each our run throughs\n",
    "    accuracy = np.mean(accuracy_list)\n",
    "    precision = np.mean(accuracy_list)\n",
    "    recall = np.mean(recall_list)\n",
    "    f1 = np.mean(f1_list)\n",
    "    kappa = np.mean(kappa_list)\n",
    "    return [accuracy, precision, recall, f1, kappa]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''K-folds for K neighbours KNeighborsClassifier '''\n",
    "def kneigh_class_k(X, Y):\n",
    "    Y = np.ravel(Y)\n",
    "    #creating an instance of the decision Tree classifier\n",
    "    knc = KNeighborsClassifier()\n",
    "    #how many 'folds' do we want\n",
    "    n_folds = 3\n",
    "    #initialize the StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits= n_folds, shuffle = True, random_state=42)\n",
    "    \n",
    "    #used to hold results from each loop \n",
    "    accuracy_list, precision_list, recall_list, f1_list, kappa_list = [], [], [], [], []\n",
    "\n",
    "    #because n_folds= 5 this should run 5 times\n",
    "    #it will return two lists of indexes - 1 for the training: X and 1 for the test: Y\n",
    "    for train, test in skf.split(X ,Y):\n",
    "        #we isolate the appropriate indexes for our training and our testing\n",
    "        X_train_k, X_test_k = X.loc[train], X.loc[test]\n",
    "        y_train_k, y_test_k = Y[train], Y[test]\n",
    "        #we enter our training data in to the instance of DecisionTreeClassifier we created\n",
    "        knc.fit(X_train_k, y_train_k)\n",
    "        #we test our model by entering our test X ie. the DF with our variables we have not used so far\n",
    "        y_pred = knc.predict(X_test_k)\n",
    "        #use the inbuilt compute_metrics to get the accuracy, precision, recall and f1 stats and add them to our container lists\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_test_k, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        kappa_score = cohen_kappa_score(y_test_k, y_pred)\n",
    "        kappa_list.append(kappa_score)\n",
    "\n",
    "    #get the average of each our run throughs\n",
    "    accuracy = np.mean(accuracy_list)\n",
    "    precision = np.mean(accuracy_list)\n",
    "    recall = np.mean(recall_list)\n",
    "    f1 = np.mean(f1_list)\n",
    "    kappa = np.mean(kappa_list)\n",
    "    return [accuracy, precision, recall, f1, kappa]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Logistic regression for Kfolds'''\n",
    "#Where X is a DF with my dependent variables and Y is a series containing encoded values\n",
    "def log_regress_k(X, Y):\n",
    "    Y = np.ravel(Y)\n",
    "    #creating an instance of the decision Tree classifier\n",
    "    lr = LogisticRegression(random_state= 42, max_iter = 1000, multi_class = 'multinomial')\n",
    "    #how many 'folds' do we want\n",
    "    n_folds = 3\n",
    "    #initialize the StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits= n_folds, shuffle = True, random_state=42)\n",
    "    \n",
    "    #used to hold results from each loop \n",
    "    accuracy_list, precision_list, recall_list, f1_list, kappa_list = [], [], [], [], []\n",
    "\n",
    "    #because n_folds= 5 this for loop should run 3 times\n",
    "    #it will return two lists of indexes - 1 for the training: X and 1 for the test: Y\n",
    "    for train, test in skf.split(X ,Y):\n",
    "        #we isolate the appropriate indexes for our training and our testing\n",
    "        X_train_k, X_test_k = X.loc[train], X.loc[test]\n",
    "        y_train_k, y_test_k = Y[train], Y[test]\n",
    "        #we enter our training data in to the instance of DecisionTreeClassifier we created\n",
    "        lr.fit(X_train_k, y_train_k)\n",
    "        #we test our model by entering our test X ie. the DF with our variables we have not used so far\n",
    "        y_pred = lr.predict(X_test_k)\n",
    "        #use the inbuilt compute_metrics to get the accuracy, precision, recall and f1 stats and add them to our container lists\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_test_k, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        kappa_score = cohen_kappa_score(y_test_k, y_pred)\n",
    "        kappa_list.append(kappa_score)\n",
    "\n",
    "    #get the average of each our run throughs\n",
    "    accuracy = np.mean(accuracy_list)\n",
    "    precision = np.mean(accuracy_list)\n",
    "    recall = np.mean(recall_list)\n",
    "    f1 = np.mean(f1_list)\n",
    "    kappa = np.mean(kappa_list)\n",
    "    return [accuracy, precision, recall, f1, kappa]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''naive bayes with K-Folds'''\n",
    "\n",
    "#Where X is a DF with my dependent variables and Y is a series containing encoded values\n",
    "def gaussian_NB_k(X, Y):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    Y = np.ravel(Y)\n",
    "    #creating an instance of the decision Tree classifier\n",
    "    gnb = GaussianNB()\n",
    "    #how many 'folds' do we want\n",
    "    n_folds = 3\n",
    "    #initialize the StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits= n_folds, shuffle = True, random_state=42)\n",
    "    \n",
    "    #used to hold results from each loop \n",
    "    accuracy_list, precision_list, recall_list, f1_list, kappa_list = [], [], [], [], []\n",
    "\n",
    "    #because n_folds= 5 this for loop should run 3 times\n",
    "    #it will return two lists of indexes - 1 for the training: X and 1 for the test: Y\n",
    "    for train, test in skf.split(X ,Y):\n",
    "        #we isolate the appropriate indexes for our training and our testing\n",
    "        X_train_k, X_test_k = X.loc[train], X.loc[test]\n",
    "        y_train_k, y_test_k = Y[train], Y[test]\n",
    "        #we enter our training data in to the instance of DecisionTreeClassifier we created\n",
    "        gnb.fit(X_train_k, y_train_k)\n",
    "        #we test our model by entering our test X ie. the DF with our variables we have not used so far\n",
    "        y_pred = gnb.predict(X_test_k)\n",
    "        #use the inbuilt compute_metrics to get the accuracy, precision, recall and f1 stats and add them to our container lists\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_test_k, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        kappa_score = cohen_kappa_score(y_test_k, y_pred)\n",
    "        kappa_list.append(kappa_score)\n",
    "\n",
    "    #get the average of each our run throughs\n",
    "    accuracy = np.mean(accuracy_list)\n",
    "    precision = np.mean(accuracy_list)\n",
    "    recall = np.mean(recall_list)\n",
    "    f1 = np.mean(f1_list)\n",
    "    kappa = np.mean(kappa_list)\n",
    "    return [accuracy, precision, recall, f1, kappa]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''support vector K with K-Folds'''\n",
    "\n",
    "#Where X is a DF with my dependent variables and Y is a series containing encoded values\n",
    "def support_vector_k(X, Y):\n",
    "    from sklearn.svm import SVC\n",
    "    Y = np.ravel(Y)\n",
    "    #creating an instance of the decision Tree classifier\n",
    "    svc = SVC()\n",
    "    #how many 'folds' do we want\n",
    "    n_folds = 3\n",
    "    #initialize the StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits= n_folds, shuffle = True, random_state=42)\n",
    "    \n",
    "    #used to hold results from each loop \n",
    "    accuracy_list, precision_list, recall_list, f1_list, kappa_list = [], [], [], [], []\n",
    "\n",
    "    #because n_folds= 5 this for loop should run 3 times\n",
    "    #it will return two lists of indexes - 1 for the training: X and 1 for the test: Y\n",
    "    for train, test in skf.split(X ,Y):\n",
    "        #we isolate the appropriate indexes for our training and our testing\n",
    "        X_train_k, X_test_k = X.loc[train], X.loc[test]\n",
    "        y_train_k, y_test_k = Y[train], Y[test]\n",
    "        #we enter our training data in to the instance of DecisionTreeClassifier we created\n",
    "        svc.fit(X_train_k, y_train_k)\n",
    "        #we test our model by entering our test X ie. the DF with our variables we have not used so far\n",
    "        y_pred = svc.predict(X_test_k)\n",
    "        #use the inbuilt compute_metrics to get the accuracy, precision, recall and f1 stats and add them to our container lists\n",
    "        accuracy, precision, recall, f1 = compute_metrics(y_test_k, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        kappa_score = cohen_kappa_score(y_test_k, y_pred)\n",
    "        kappa_list.append(kappa_score)\n",
    "\n",
    "    #get the average of each our run throughs\n",
    "    accuracy = np.mean(accuracy_list)\n",
    "    precision = np.mean(accuracy_list)\n",
    "    recall = np.mean(recall_list)\n",
    "    f1 = np.mean(f1_list)\n",
    "    kappa = np.mean(kappa_list)\n",
    "    return [accuracy, precision, recall, f1, kappa]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df2 = pd.DataFrame(index=['accuracy', 'precision', 'recall', 'f1', 'kappa'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dec_tree_kfold(ml_df2.iloc[:,:-1], ml_df2.iloc[:, -1:])\n",
    "\n",
    "comparison_df2['Decision_tree_K'] = dec_tree_kfold(ml_df2.iloc[:,:-1], ml_df2.iloc[:, -1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df2['Random_forest_K'] = random_forest_kfold(ml_df2.iloc[:,:-1], ml_df2.iloc[:, -1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df2['K-neighbours_K'] = kneigh_class_k(ml_df2.iloc[:,:-1], ml_df2.iloc[:, -1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df2['Logistic_regression_K'] = log_regress_k(ml_df2.iloc[:,:-1], ml_df2.iloc[:, -1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df2['gaussian_NB_k'] = gaussian_NB_k(ml_df2.iloc[:,:-1], ml_df2.iloc[:, -1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df2['support_vector_K'] = support_vector_k(ml_df2.iloc[:,:-1], ml_df2.iloc[:, -1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Evaluation metrics for algorithims\n",
    "### Decision_tree_K, Random_forest_K, K-neighbours_K,Logistic_regression_K, gaussian_NB_k, support_vector_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df3 = comparison_df2.style.apply(highlight_max, axis = 1)\n",
    "display(comparison_df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of algorithms\n",
    "## removing features in consequtive increasing correlation score without returning removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
